{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanadinh1203/Data-Analytics-/blob/main/s5332240_Fashion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKmK8jXPH8sh"
      },
      "source": [
        "# Image Classification by MLP - Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgzUsSpWH8si"
      },
      "source": [
        "In this exercise, we will try to use a neural network on a simple classification task: classifying images of clothes into 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNc-wT81H8si"
      },
      "source": [
        "We will first download the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhB39T-HH8si",
        "outputId": "69087f07-9e65-4a6b-f20b-94bfe4a7afd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "#TODO: load dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#TODO: Resample the dataset if needed\n",
        "# X_train =\n",
        "# y_train = ...\n",
        "# X_test = ...\n",
        "# y_test = ...\n",
        "\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O25MxLK1H8sj"
      },
      "source": [
        "This dataset contains 10 classes:\n",
        "* 0:\tT-shirt/top\n",
        "* 1:\tTrouser\n",
        "* 2:\tPullover\n",
        "* 3:\tDress\n",
        "* 4:\tCoat\n",
        "* 5:\tSandal\n",
        "* 6:\tShirt\n",
        "* 7:\tSneaker\n",
        "* 8:\tBag\n",
        "* 9:\tAnkle boot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IL1lcjlH8sj"
      },
      "source": [
        "Now begin by exploring the data. Try to display some images with the associated label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "pzh1yK9BH8sj",
        "outputId": "2cb54d0a-8294-4fe6-cd09-47597f9f2d77"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# TODO: Explore the data, display some input images\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "label_class = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "\n",
        "# np.random.seed(0)\n",
        "idx = np.random.randint(X_train.shape[0])\n",
        "\n",
        "plt.imshow(X_train[idx], cmap=\"gray_r\")\n",
        "plt.title(label_class[y_train[idx]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bVAXRG4H8sj"
      },
      "source": [
        "**Before going further**: what methods could you use to perform such a classification task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhwMf3WKH8sj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwE5TNwaH8sj"
      },
      "source": [
        "The first method you will try is using neural networks. First step is the data preparation: data rescaling, label preparation.\n",
        "\n",
        "Hint: you can use the Keras function `to_categorical`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeOAowMQH8sj",
        "outputId": "ae30ca8b-d178-4f0c-b0b3-9a49db99355f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000, 784)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Make the data preparation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_cat = to_categorical(y_train, num_classes=10)\n",
        "y_test_cat = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "X_train_norm = X_train/255\n",
        "X_test_norm = X_test/255\n",
        "\n",
        "# TODO: reshape the image data (2D array) into input 1D array for a neural network\n",
        "print(np.shape(X_train_norm))\n",
        "X_train_norm = X_train_norm.reshape(X_train.shape[0],np.prod(X_train_norm.shape[1:]))\n",
        "print(np.shape(X_train_norm))\n",
        "X_test_norm = X_test_norm.reshape(X_test.shape[0], np.prod(X_test_norm.shape[1:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDCEp_u-H8sj"
      },
      "source": [
        "Next step: model building with Keras. Build your neural network architecture. At first, I would recommend a light architecture: no more than 2 hidden layers, with about 10 units per layer. Put that model into a function, so that you can reuse it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rml3XjrH8sj",
        "outputId": "75bb93eb-8cee-441e-d250-27f90d832802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                7850      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7960 (31.09 KB)\n",
            "Trainable params: 7960 (31.09 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# TODO: Build your model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def my_model(input_dim):\n",
        "    # Create the Sequential object\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add 2 dense layers with 10 neurons each using sigmoid or relu activation\n",
        "    model.add(Dense(10, input_dim=input_dim, activation=\"sigmoid\"))\n",
        "\n",
        "    # Add the output layer with one unit: the predicted result\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "my_model(X_train_norm.shape[1]).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SoPoL3iH8sj"
      },
      "source": [
        "Now compile and fit your model on your training data. Since this is a multiclass classification, the loss is not `binary_crossentropy` anymore, but `categorical_crossentropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqkcvc06H8sj",
        "outputId": "14b3275d-0bce-4421-fd18-d88315e86062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 1.3996 - accuracy: 0.6549\n",
            "Epoch 2/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.8168 - accuracy: 0.7945\n",
            "Epoch 3/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.6336 - accuracy: 0.8171\n",
            "Epoch 4/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.5513 - accuracy: 0.8296\n",
            "Epoch 5/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.5051 - accuracy: 0.8367\n",
            "Epoch 6/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4759 - accuracy: 0.8423\n",
            "Epoch 7/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4547 - accuracy: 0.8466\n",
            "Epoch 8/100\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.4394 - accuracy: 0.8506\n",
            "Epoch 9/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4264 - accuracy: 0.8530\n",
            "Epoch 10/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4167 - accuracy: 0.8559\n",
            "Epoch 11/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4092 - accuracy: 0.8575\n",
            "Epoch 12/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4022 - accuracy: 0.8594\n",
            "Epoch 13/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3961 - accuracy: 0.8615\n",
            "Epoch 14/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3905 - accuracy: 0.8625\n",
            "Epoch 15/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3868 - accuracy: 0.8634\n",
            "Epoch 16/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3824 - accuracy: 0.8656\n",
            "Epoch 17/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3790 - accuracy: 0.8661\n",
            "Epoch 18/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3753 - accuracy: 0.8672\n",
            "Epoch 19/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3721 - accuracy: 0.8685\n",
            "Epoch 20/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3691 - accuracy: 0.8696\n",
            "Epoch 21/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3670 - accuracy: 0.8705\n",
            "Epoch 22/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3652 - accuracy: 0.8707\n",
            "Epoch 23/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3621 - accuracy: 0.8719\n",
            "Epoch 24/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3609 - accuracy: 0.8732\n",
            "Epoch 25/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3585 - accuracy: 0.8731\n",
            "Epoch 26/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3575 - accuracy: 0.8729\n",
            "Epoch 27/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3556 - accuracy: 0.8733\n",
            "Epoch 28/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3549 - accuracy: 0.8743\n",
            "Epoch 29/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3523 - accuracy: 0.8753\n",
            "Epoch 30/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3510 - accuracy: 0.8747\n",
            "Epoch 31/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3495 - accuracy: 0.8766\n",
            "Epoch 32/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3482 - accuracy: 0.8774\n",
            "Epoch 33/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3469 - accuracy: 0.8773\n",
            "Epoch 34/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3445 - accuracy: 0.8786\n",
            "Epoch 35/100\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3448 - accuracy: 0.8784\n",
            "Epoch 36/100\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3439 - accuracy: 0.8779\n",
            "Epoch 37/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3424 - accuracy: 0.8788\n",
            "Epoch 38/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3412 - accuracy: 0.8791\n",
            "Epoch 39/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3405 - accuracy: 0.8791\n",
            "Epoch 40/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3386 - accuracy: 0.8795\n",
            "Epoch 41/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3386 - accuracy: 0.8800\n",
            "Epoch 42/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3372 - accuracy: 0.8799\n",
            "Epoch 43/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3356 - accuracy: 0.8808\n",
            "Epoch 44/100\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3352 - accuracy: 0.8809\n",
            "Epoch 45/100\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3347 - accuracy: 0.8813\n",
            "Epoch 46/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3330 - accuracy: 0.8808\n",
            "Epoch 47/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3325 - accuracy: 0.8822\n",
            "Epoch 48/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3325 - accuracy: 0.8823\n",
            "Epoch 49/100\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3316 - accuracy: 0.8823\n",
            "Epoch 50/100\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.3309 - accuracy: 0.8824\n",
            "Epoch 51/100\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3301 - accuracy: 0.8822\n",
            "Epoch 52/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3291 - accuracy: 0.8828\n",
            "Epoch 53/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3281 - accuracy: 0.8837\n",
            "Epoch 54/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3276 - accuracy: 0.8830\n",
            "Epoch 55/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3260 - accuracy: 0.8836\n",
            "Epoch 56/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3270 - accuracy: 0.8832\n",
            "Epoch 57/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3254 - accuracy: 0.8845\n",
            "Epoch 58/100\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3249 - accuracy: 0.8840\n",
            "Epoch 59/100\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.3244 - accuracy: 0.8846\n",
            "Epoch 60/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3243 - accuracy: 0.8844\n",
            "Epoch 61/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3232 - accuracy: 0.8845\n",
            "Epoch 62/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3221 - accuracy: 0.8859\n",
            "Epoch 63/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3219 - accuracy: 0.8858\n",
            "Epoch 64/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3211 - accuracy: 0.8861\n",
            "Epoch 65/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3209 - accuracy: 0.8860\n",
            "Epoch 66/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3200 - accuracy: 0.8857\n",
            "Epoch 67/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3198 - accuracy: 0.8866\n",
            "Epoch 68/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3200 - accuracy: 0.8856\n",
            "Epoch 69/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3187 - accuracy: 0.8861\n",
            "Epoch 70/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3180 - accuracy: 0.8864\n",
            "Epoch 71/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3174 - accuracy: 0.8874\n",
            "Epoch 72/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3166 - accuracy: 0.8862\n",
            "Epoch 73/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3164 - accuracy: 0.8869\n",
            "Epoch 74/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3167 - accuracy: 0.8872\n",
            "Epoch 75/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3153 - accuracy: 0.8878\n",
            "Epoch 76/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3161 - accuracy: 0.8873\n",
            "Epoch 77/100\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3149 - accuracy: 0.8884\n",
            "Epoch 78/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3141 - accuracy: 0.8878\n",
            "Epoch 79/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3141 - accuracy: 0.8887\n",
            "Epoch 80/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3130 - accuracy: 0.8889\n",
            "Epoch 81/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3124 - accuracy: 0.8892\n",
            "Epoch 82/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3123 - accuracy: 0.8884\n",
            "Epoch 83/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3119 - accuracy: 0.8885\n",
            "Epoch 84/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3114 - accuracy: 0.8888\n",
            "Epoch 85/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3112 - accuracy: 0.8892\n",
            "Epoch 86/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3115 - accuracy: 0.8891\n",
            "Epoch 87/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3102 - accuracy: 0.8892\n",
            "Epoch 88/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3099 - accuracy: 0.8893\n",
            "Epoch 89/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3103 - accuracy: 0.8892\n",
            "Epoch 90/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3092 - accuracy: 0.8899\n",
            "Epoch 91/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3096 - accuracy: 0.8894\n",
            "Epoch 92/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3082 - accuracy: 0.8905\n",
            "Epoch 93/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3076 - accuracy: 0.8900\n",
            "Epoch 94/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3077 - accuracy: 0.8905\n",
            "Epoch 95/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3073 - accuracy: 0.8904\n",
            "Epoch 96/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3070 - accuracy: 0.8906\n",
            "Epoch 97/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3081 - accuracy: 0.8901\n",
            "Epoch 98/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3068 - accuracy: 0.8906\n",
            "Epoch 99/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3065 - accuracy: 0.8906\n",
            "Epoch 100/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3050 - accuracy: 0.8910\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f52f64a8b20>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "#https://stackoverflow.com/questions/53014306/error-15-initializing-libiomp5-dylib-but-found-libiomp5-dylib-already-initial\n",
        "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "# TODO: Compile and fit your model\n",
        "model = my_model(X_train_norm.shape[1])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_norm, y_train_cat, epochs=100, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryhJtw4OH8sj"
      },
      "source": [
        "Once your model has been trained, compute the accuracy (and other metrics if you want) on the train and test dataset.\n",
        "\n",
        "Be careful, Keras returns softmax output (so an array of 10 values between 0 and 1, for which the sum is equal to 1). To compute correctly the accuracy, you have to convert that array into a categorical array with zeros and a 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48nRmy0XH8sj",
        "outputId": "ea7d0b99-ccdf-443f-bdd4-a4c92a2858a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on train with NN: 0.8912500143051147\n",
            "accuracy on test with NN: 0.8499000072479248\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compute the accuracy of your model\n",
        "print('accuracy on train with NN:', model.evaluate(X_train_norm, y_train_cat, verbose=0)[1])\n",
        "print('accuracy on test with NN:', model.evaluate(X_test_norm, y_test_cat, verbose=0)[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ni2GvCH8sj"
      },
      "source": [
        "What do you think of those results? Can you improve it by changing the number of layers? Of units per layer? The number of epochs? The activation functions?\n",
        "\n",
        "You should try!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DEjcXWlH8sk"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oheAKgmyH8sk"
      },
      "source": [
        "In order to compare your results with more traditional machine learning methods, you will do this work with another method: a PCA followed by a classification model (of your choice). Of course, you can perform hyperparameter optimization using a gridsearch on that model!\n",
        "\n",
        "Fit your model and display the performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gqFXBcHYH8sk"
      },
      "outputs": [],
      "source": [
        "# TODO: Redo the classification with PCA and classification model\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.9)\n",
        "\n",
        "pca.fit(X_train_norm)\n",
        "X_train_pca = pca.transform(X_train_norm)\n",
        "X_test_pca = pca.transform(X_test_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K68ExIiAH8sk",
        "outputId": "c00a54e3-609d-4a84-aed2-360fc867ca65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score with RF on train 1.0\n",
            "score with RF on train 0.86\n"
          ]
        }
      ],
      "source": [
        "# TODO: use any classifier you want\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "rf.fit(X_train_pca, y_train)\n",
        "\n",
        "print('score with RF on train', rf.score(X_train_pca, y_train))\n",
        "print('score with RF on train', rf.score(X_test_pca, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrGkpqhmH8sk"
      },
      "source": [
        "Are the performances different? Can you explain why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-8tBn6yH8sk"
      },
      "source": [
        "If you still have time, you could try to use scikit-learn's `Pipeline` to perform the hyperparameter optimization jointly on the PCA and the classification model. This might improve your performances."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}